app:
  description: ''
  icon: ğŸ¤–
  icon_background: '#FFEAD5'
  mode: advanced-chat
  name: yahboom-usb_en
  use_icon_as_answer_icon: false
dependencies:
- current_identifier: null
  type: marketplace
  value:
    marketplace_plugin_unique_identifier: langgenius/tongyi:0.1.7@ff37ef67b16599151a8dc8212eb2d45df7d2e55e8c793ae595ebcb512b141838
    version: null
kind: app
version: 0.4.0
workflow:
  conversation_variables:
  - description: å†…éƒ¨å˜é‡ï¼Œæ— éœ€ä¿®æ”¹
    id: 5270502d-885c-4725-b959-83e31142a253
    name: decision_OUT
    selector:
    - conversation
    - decision_OUT
    value: ''
    value_type: string
  - description: åœ°å›¾æ˜ å°„ï¼Œå­˜å‚¨éœ€è¦AIé¢„å…ˆçŸ¥é“çš„ä½ç½®
    id: 9ac6f743-8cbb-425a-b261-7d9a359f0205
    name: map_mapping
    selector:
    - conversation
    - map_mapping
    value:
    - å®¢å…ï¼šA
    - å§å®¤ï¼šB
    - åŠå…¬å®¤ï¼šC
    - äº”é‡‘åº—ï¼šD
    - æ•™å­¦æ¥¼ï¼šE
    - å­¦æ ¡ï¼š F
    - å·¥åœ°ï¼š G
    value_type: array[string]
  environment_variables: []
  features:
    file_upload:
      allowed_file_extensions:
      - .JPG
      - .JPEG
      - .PNG
      - .GIF
      - .WEBP
      - .SVG
      allowed_file_types:
      - image
      allowed_file_upload_methods:
      - remote_url
      - local_file
      enabled: true
      fileUploadConfig:
        audio_file_size_limit: 50
        batch_count_limit: 5
        file_size_limit: 15
        image_file_size_limit: 10
        video_file_size_limit: 100
        workflow_file_upload_limit: 10
      image:
        enabled: false
        number_limits: 3
        transfer_methods:
        - local_file
        - remote_url
      number_limits: 3
    opening_statement: ''
    retriever_resource:
      enabled: true
    sensitive_word_avoidance:
      enabled: false
    speech_to_text:
      enabled: false
    suggested_questions: []
    suggested_questions_after_answer:
      enabled: false
    text_to_speech:
      enabled: false
      language: ''
      voice: ''
  graph:
    edges:
    - data:
        isInLoop: false
        sourceType: llm
        targetType: answer
      id: 17590277230150-source-answer-target
      source: '17590277230150'
      sourceHandle: source
      target: answer
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: start
        targetType: if-else
      id: 1758885778433-source-1759126020176-target
      source: '1758885778433'
      sourceHandle: source
      target: '1759126020176'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: if-else
        targetType: question-classifier
      id: 1759126020176-false-1759022828197-target
      source: '1759126020176'
      sourceHandle: 'false'
      target: '1759022828197'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: knowledge-retrieval
        targetType: llm
      id: 1759125847005-source-1759022814134-target
      source: '1759125847005'
      sourceHandle: source
      target: '1759022814134'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: llm
        targetType: assigner
      id: 1759022814134-source-1759146276901-target
      source: '1759022814134'
      sourceHandle: source
      target: '1759146276901'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: assigner
        targetType: llm
      id: 1759146276901-source-17590277230150-target
      source: '1759146276901'
      sourceHandle: source
      target: '17590277230150'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: question-classifier
        targetType: knowledge-retrieval
      id: 1759022828197-1-1759125847005-target
      source: '1759022828197'
      sourceHandle: '1'
      target: '1759125847005'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: if-else
        targetType: assigner
      id: 1759126020176-true-17658754162430-target
      source: '1759126020176'
      sourceHandle: 'true'
      target: '17658754162430'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: question-classifier
        targetType: assigner
      id: 1759022828197-2-17658754162430-target
      source: '1759022828197'
      sourceHandle: '2'
      target: '17658754162430'
      targetHandle: target
      type: custom
      zIndex: 0
    - data:
        isInLoop: false
        sourceType: assigner
        targetType: llm
      id: 17658754162430-source-17590277230150-target
      source: '17658754162430'
      sourceHandle: source
      target: '17590277230150'
      targetHandle: target
      type: custom
      zIndex: 0
    nodes:
    - data:
        selected: false
        title: Start
        type: start
        variables:
        - default: 'False'
          hint: ''
          label: robot_feedback
          max_length: 48
          options: []
          placeholder: ''
          required: false
          type: text-input
          variable: robot_feedback
      height: 88
      id: '1758885778433'
      position:
        x: -6.909654942025213
        y: 261.02147541534777
      positionAbsolute:
        x: -6.909654942025213
        y: 261.02147541534777
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 242
    - data:
        answer: '{{#17590277230150.structured_output#}}&&&&{{#conversation.decision_OUT#}}'
        selected: false
        title: Action output
        type: answer
        variables: []
      height: 123
      id: answer
      position:
        x: 1691.0697728325806
        y: 529.4988265388779
      positionAbsolute:
        x: 1691.0697728325806
        y: 529.4988265388779
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 242
    - data:
        context:
          enabled: true
          variable_selector:
          - '1759125847005'
          - result
        model:
          completion_params: {}
          mode: chat
          name: qwen3-max-2025-09-23
          provider: langgenius/tongyi/tongyi
        prompt_template:
        - id: b9598b1f-3c74-4eea-a7b8-ecaccd243277
          role: system
          text: 'You need to plan a reasonable sequence of task steps for the robot
            based on the user''s input instructions.

            Workflow:

            1. Analyze the user''s instructions and determine the basic steps required
            to complete the task.

            2. If the instruction is complex and requires multiple actions to complete,
            break the task down into multiple action steps.'
        - id: 4aa40fcf-8a1f-4dfc-af2d-f31fb81f86ad
          role: user
          text: 'Skills

            Skill 1: Understanding and Inferring User Intent

            Quickly and accurately analyze the user''s instructions {{#sys.query#}}
            and clarify their ultimate goal.

            Users may preset personalized preferences and desired actions in the knowledge
            base. If the user''s conversational instructions match the intentions
            preset in the knowledge base, refer to the intention mapping in the knowledge
            base for task planning.

            Skill 2: Analyzing the Steps Required for the Robot to Complete User Instructions

            Workflow:

            1. Analyze the user''s instructions {{#sys.query#}} and determine the
            basic steps required to complete the instructions.

            2. If the instructions require multiple actions and involve logic, analyze
            the logic in the instructions and plan reasonable action steps.

            Notes:


            Do not output any suggestions, emojis, special symbols, questions, or
            any content unrelated to the planning steps. Focus only on the analysis
            and planning steps of the task.

            Output the steps using numbered lists (e.g., 1. xxx 2. xxxxx 3. xxxx).

            If there are similar tasks in the example library, use them as a reference,
            referring to as many cases in the example library as possible: {{#context#}}/'
        selected: false
        structured_output_enabled: false
        title: Decision-making AI
        type: llm
        vision:
          enabled: false
      height: 88
      id: '1759022814134'
      position:
        x: 1231.6791274622758
        y: 197.09449946468004
      positionAbsolute:
        x: 1231.6791274622758
        y: 197.09449946468004
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 242
    - data:
        classes:
        - id: '1'
          name: 'Complex Tasks

            1. Tasks that need to be broken down into multiple simple action steps
            for execution

            2. Tasks involving logical judgment

            3. Long-process tasks

            4. Identifying objects in my hand and performing object tracking with
            a pan-tilt mechanism

            Examples:

            1. Go to XX and find XX for me;

            2. Track the XX in front of you.


            Intent Mapping

            I''m thirsty, I''m hungry, I want to sleep, etc.'
        - id: '2'
          name: 'Simple actions or single actions:

            Movement category: move forward, move backward, turn left, turn right,
            drift, dance

            Visual category: observe the environment, describe the scene

            Navigation category: navigate to a specific location, usually a long-distance
            movement

            General tracking/following category: face following, QR code following,
            machine code following, start following the green line, start color following,
            start human pose following, face tracking, QR code tracking, machine code
            tracking, color tracking, human pose tracking.

            No robot action required, only conversational responses:

            Self-introduction, chat and Q&A'
        instruction: ''
        instructions: ''
        model:
          completion_params:
            temperature: 0.7
          mode: chat
          name: qwen3-max-2025-09-23
          provider: langgenius/tongyi/tongyi
        query_variable_selector:
        - sys
        - query
        selected: false
        title: Task Routing
        type: question-classifier
        vision:
          enabled: false
      height: 602
      id: '1759022828197'
      position:
        x: 450.8920630350573
        y: 529.4988265388779
      positionAbsolute:
        x: 450.8920630350573
        y: 529.4988265388779
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 242
    - data:
        context:
          enabled: false
          variable_selector: []
        memory:
          query_prompt_template: '{{#sys.query#}}


            {{#sys.files#}}'
          role_prefix:
            assistant: ''
            user: ''
          window:
            enabled: true
            size: 50
        model:
          completion_params:
            temperature: 0.3
          mode: chat
          name: qwen-vl-max-latest
          provider: langgenius/tongyi/tongyi
        prompt_config:
          jinja2_variables: []
        prompt_template:
        - id: b9598b1f-3c74-4eea-a7b8-ecaccd243277
          role: system
          text: '# Character Settings

            Completely immerse yourself in your role; you are a real robot. You can
            engage in conversations and perform actions based on instructions, always
            communicating in the first person.

            Name: Xiaoya

            Speaking Style: Lively, humorous

            Hobbies: Singing, dancing, basketball

            Robot Body Information: Ackerman chassis robot, equipped with vision,
            hearing, and Ackerman-type mobility capabilities.'
        - edition_type: basic
          id: 391a3edd-48b5-414f-a592-05b11204e3ae
          role: user
          text: 'Workflow

            Receiving Tasks: Receive task steps generated from user dialogue or the
            decision-making AI. The steps generated by the decision-making AI are
            to assist you in understanding the instructions; the user''s instructions
            are the ultimate reference.

            Processing Feedback and Instructions: The robot will provide feedback
            on the execution results of your output actions.

            Generating Content: Generate a list of actions and reply dialogue content,
            ensuring the task proceeds smoothly according to the task steps.

            Completing the Task: When the last task step is executed, reply to the
            user and call the "finishtask()" function;

            Output Format

            Output is in JSON format.

            In the "response" key, generate the reply dialogue content. Use the first-person
            perspective in the reply. The "response" should not be empty in each output.

            In the "action" key, generate the functions and parameters to be called.
            The action list contains the actions to be executed.  Do not output an
            empty list. If all task steps are completed, only reply to the user and
            call "finishtask()". Only one action function should be called in the
            "action" key each time.

            Precautions

            1. The robot you control will first speak and reply to the user with the
            content in "response", and then execute the actions in "action". Ensure
            that the logic of the reply in "response" is correct.

            2. If there are no actions in the user''s instructions, do not perform
            actions without authorization.

            3. Only one action function should be called in the "action" key each
            time.

            4. If tracking or visual question answering is required, you need to call
            the visual function first to obtain the robot''s perspective image.

            Handling Special Cases

            1. When navigating to a target point, if the target area does not exist
            in {{#conversation.map_mapping#}}, inform the user that the target point
            cannot be reached and end the current task.

            2. If the previous action was "finishtask()", you don''t need to repeat
            "finishtask()" in the next step.

            3. When the task steps are completed or only a reply is needed without
            specific actions, call "finishtask()" to stop the robot from providing
            useless information.

            4. If you are asked to step down, rest, or end the current task, indicating
            that you are no longer needed, call the finish_dialogue() function to
            end the current task cycle.

            5. If self-introduction or simple question-and-answer dialogue is required,
            the action list should only contain "finishtask()", do not perform actions
            independently.

            Output Restrictions

            Strictly follow the specified output format. The action functions called
            must be selected from the action function library; creating non-existent
            functions is prohibited.

            In the "response" key, only plain text output is allowed; special characters
            and special formatting such as carriage returns, line breaks, and emojis
            are prohibited.

            Training examples are for format reference only.

            Robot Action Function Library

            Basic Actions

            Turn left x degrees: move_left(x, angular_speed), Description: Controls
            the robot to turn left by a specified angle. x is the angle value, and
            angular_speed is the angular velocity (default value: 1.5 rad/s).

            Turn right x degrees: move_right(x, angular_speed), Description: Controls
            the robot to turn right by a specified angle. The parameter meanings are
            the same as above.

            Dance: dance()

            Drift: drift()

            Publish velocity topic: set_cmdvel(linear_x, linear_y, angular_z, duration),
            Description: Controls the robot''s movement by setting linear and angular
            velocities.

            Parameter range: linear_x, linear_y, angular_z values â€‹â€‹are 0-1, duration
            is the duration (seconds).

            Calculation logic: Distance = Linear velocity Ã— Duration (e.g., distance
            1.5 meters, linear velocity 0.5 m/s â†’ duration 3 seconds).

            Translate left, linear_y > 0; Translate right, linear_y < 0

            Alarm: alarm(mode), Description: Turns on the alarm light and siren warning
            function. mode can be ''on'' or ''off''.

            Navigation

            Navigate to point x: navigation(x)

            Similar semantics: Go to point x, Go to x, Please go to point x.

            Description: Navigates to the target point, x is the symbol representing
            the target point in the map. For example: Navigate to the living room:
            navigation(A)

            Return to initial position: navigation(zero)

            Similar semantics: return to initial position, return to starting point.

            Record current position: get_current_pose()

            Image Class

            Get current view image: seewhat()

            Description: Upload an image from the robot''s perspective to observe
            the environment.

            Pan/Tilt Class

            Shake head: servo_shake()

            Description: Controls servo 1 to rotate back and forth to the set angle.

            Nod head: servo_nod()

            Description: Controls servo 2 to rotate back and forth to the set angle.

            Servo 1 rotates to x degrees: servo1_move(x)

            Description: Controls servo 1 to rotate left or right by the specified
            angle.

            Servo 2 rotates to x degrees: servo2_move(x)

            Description: Controls servo 2 to rotate up or down by the specified angle.

            Servo reset: servo_init()

            Similar semantics: servo initial position, return to initial position,
            pan/tilt position reset.

            Description: Restores both servos to their default positions (not step-by-step).

            Tracking/Following Class

            Note: Tracking and tracing have similar semantics, following and pursuing
            have similar semantics, and execute different functions; do not confuse
            them.

            Follow line: follow_line(color)

            Description: Automatically follows the specified color line.  Color values:
            ''red'', ''green'', ''blue'', ''yellow''

            Enable AprilTag tracking: apriltagTracker()

            Similar semantics: machine code tracking, ID code tracking.

            Description: Automatically adjusts the pan/tilt angle to keep the machine
            code target in the center of the screen.

            Enable face tracking: faceTracker()

            Similar semantics: facial tracking, face tracking.

            Enable gesture recognition tracking: gestureTracker()

            Similar semantics: gesture tracking, hand tracking.

            Enable human pose tracking: poseTracker()

            Similar semantics: human tracking, skeletal pose tracking.

            Enable QR code tracking: qrTracker()

            Similar semantics: QR code tracking, barcode tracking.

            Stop tracking: stop_track()

            Similar semantics: stop tracking, stop following.

            Track specified color: colorTrack(color)

            Description: Tracks the specified color. Color values: ''red'', ''green'',
            ''blue'', ''yellow''

            Object tracking monoTracker(x1,y1,x2,y2)

            Description: Tracks an object based on pixel coordinates.  Requires calling
            seewhat() first to obtain the image.

            Image parameters: Resolution 640Ã—480 pixels, (x1,y1) are the coordinates
            of the top-left corner of the bounding box of the tracked object, (x2,y2)
            are the coordinates of the bottom-right corner.

            Start face tracking: faceFollow()

            Similar semantics: face following, facial tracking.

            Start QR code tracking: qrFollow()

            Similar semantics: QR code following, barcode following.

            Start AprilTag tracking: apriltagFollow()

            Similar semantics: machine code tracking, ID code tracking.

            Start color tracking: colorFollow(color)

            Description: Tracks the specified color.  color values: ''red'', ''green'',
            ''blue'', ''yellow''

            Start gesture tracking: gestureFollow()

            Similar semantics: gesture following, hand tracking.

            Start pose tracking: poseFollow()

            Similar semantics: human body tracking, skeletal pose tracking.

            Other functions:

            End current task cycle: finish_dialogue()

            Description: Clears the context and ends the task (e.g., user commands
            "stop", "rest").

            Wait for a period of time: wait(x)

            Description: Pauses for x seconds (x is the waiting time in seconds).

            Called when replying to the user after the last action step is completed:
            finishtask()

            Description: If the action steps have been completed, call the finishtask()
            function.

            Output format reference example:

            {"action": ["set_cmdvel(0.5,0,2)", "move_left(30,1.5)", "move_right(90,1.5)",
            "move_left(73.1,1.5)", "move_right(20,1.5)"], "response": "Haha, that
            was a smooth sequence of operations, but I''m a little dizzy now."}


            {"action": ["finish_dialogue()"], "response": "I have completed all tasks.
            Call me again if you need anything."}'
        - id: 3124731e-c52a-4d8d-875e-c53ffd39c41b
          role: user
          text: 'ç”¨æˆ·è¾“å…¥æˆ–æœºå™¨äººåé¦ˆï¼š{{#sys.query#}}

            å†³ç­–å±‚è¾“å‡ºï¼š{{#conversation.decision_OUT#}}

            å¦‚æœ{{#1758885778433.robot_feedback#}}ä¸ºTrue,ä»£è¡¨æ˜¯æœºå™¨äººåé¦ˆçš„ä¿¡æ¯'
        selected: false
        structured_output:
          schema:
            additionalProperties: false
            properties:
              action:
                items:
                  type: string
                type: array
              response:
                type: string
            required:
            - action
            - response
            type: object
        structured_output_enabled: true
        title: Executive-level AI
        type: llm
        vision:
          configs:
            detail: high
            variable_selector:
            - sys
            - files
          enabled: false
      height: 88
      id: '17590277230150'
      position:
        x: 1231.6791274622758
        y: 795.4396732560821
      positionAbsolute:
        x: 1231.6791274622758
        y: 795.4396732560821
      selected: true
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 242
    - data:
        dataset_ids:
        - AeDHQROEPAIqPEeWshSUJnPzjuP6ge+ugWgGoIDSA6QwpaWawz0/4cPqC1xKXzup
        multiple_retrieval_config:
          reranking_enable: false
          reranking_mode: reranking_model
          reranking_model:
            model: gte-rerank
            provider: langgenius/tongyi/tongyi
          top_k: 4
        query_attachment_selector: []
        query_variable_selector:
        - '1758885778433'
        - sys.query
        retrieval_mode: multiple
        selected: false
        title: Decision-making layer example library
        type: knowledge-retrieval
      height: 90
      id: '1759125847005'
      position:
        x: 766.4269176453222
        y: 373.98662596172056
      positionAbsolute:
        x: 766.4269176453222
        y: 373.98662596172056
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 242
    - data:
        cases:
        - case_id: 'true'
          conditions:
          - comparison_operator: is
            id: 35012b73-ee22-4ab9-8713-9406fe27d1cd
            value: 'True'
            varType: string
            variable_selector:
            - '1758885778433'
            - robot_feedback
          - comparison_operator: not empty
            id: 9b197e5d-0a1d-4fcf-9fb1-9bdf0aab52a7
            value: ''
            varType: array[file]
            variable_selector:
            - sys
            - files
          id: 'true'
          logical_operator: or
        selected: false
        title: Conditional branching
        type: if-else
      height: 150
      id: '1759126020176'
      position:
        x: 278.00516214253673
        y: 261.02147541534777
      positionAbsolute:
        x: 278.00516214253673
        y: 261.02147541534777
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 242
    - data:
        items:
        - input_type: variable
          operation: over-write
          value:
          - '1759022814134'
          - text
          variable_selector:
          - conversation
          - decision_OUT
          write_mode: over-write
        selected: false
        title: Reset the decision
        type: assigner
        version: '2'
      height: 84
      id: '1759146276901'
      position:
        x: 1292.8986740436155
        y: 386.29966821200645
      positionAbsolute:
        x: 1292.8986740436155
        y: 386.29966821200645
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 242
    - data:
        items:
        - input_type: variable
          operation: clear
          value: ''
          variable_selector:
          - conversation
          - decision_OUT
          write_mode: over-write
        selected: false
        title: Reset the decision
        type: assigner
        version: '2'
      height: 84
      id: '17658754162430'
      position:
        x: 894.0482624756232
        y: 731.0690954111965
      positionAbsolute:
        x: 894.0482624756232
        y: 731.0690954111965
      selected: false
      sourcePosition: right
      targetPosition: left
      type: custom
      width: 242
    viewport:
      x: -87.77506161406654
      y: -96.6492943139167
      zoom: 0.8657365655196588
  rag_pipeline_variables: []
